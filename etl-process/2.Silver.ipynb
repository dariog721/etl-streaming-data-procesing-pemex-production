{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c85fd345-2be6-4b37-a0c9-40500346b216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import functions as F, types as T, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b9ce38-f638-490e-85b9-d2db4ab8a2f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_fecha_col_batch(df, fecha_col=\"FECHA\"):\n",
    "    return (df\n",
    "            .withColumn(\"FECHA\", F.to_date(\"FECHA\", \"yyyy-MM-dd\"))\n",
    "    )\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "def unpivot_via_explode_safe(df, fecha_col=\"FECHA\"):\n",
    "\n",
    "    exclude = {fecha_col.lower(), \"_rescued_data\", \"rescued_data\", \"ingestion_date\"}\n",
    "    value_cols = [c for c in df.columns if c.lower() not in exclude]\n",
    "\n",
    "\n",
    "    pairs = F.array(*[\n",
    "        F.struct(\n",
    "            F.lit(c).alias(\"POZO\"),\n",
    "            F.col(c).cast(T.DoubleType()).alias(\"PRODUCCION_MBD\")\n",
    "        )\n",
    "        for c in value_cols\n",
    "    ])\n",
    "\n",
    "\n",
    "    return (df\n",
    "        .select(F.col(fecha_col).alias(\"FECHA\"), F.explode(pairs).alias(\"s\"))\n",
    "        .select(\"FECHA\", \"s.POZO\", \"s.PRODUCCION_MBD\"))\n",
    "\n",
    "\n",
    "def add_basic_flags(df):\n",
    "    return (df\n",
    "      .withColumn(\"PRODUCCION_MBD\", F.col(\"PRODUCCION_MBD\").cast(T.DoubleType()))\n",
    "      .withColumn(\"FLAG_NULL\", F.when(F.col(\"PRODUCCION_MBD\").isNull(), 1).otherwise(0))\n",
    "      .withColumn(\"FLAG_ZERO\", F.when(F.col(\"PRODUCCION_MBD\") == 0, 1).otherwise(0))\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ef9e59-a9ce-4b60-b07b-c4d2c7148fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_table = \"workspace.pmx_etl.silver_pemex_prod\"\n",
    "checkpoint_silver = \"/Volumes/workspace/pmx_etl/checkpoints_silver\"\n",
    "def process_silver(microDF, batchId: int):\n",
    "\n",
    "    df = normalize_fecha_col_batch(microDF, \"FECHA\")\n",
    "    df = unpivot_via_explode_safe(df, \"FECHA\")\n",
    "    df = add_basic_flags(df).withColumn(\"DATA_SOURCE\", F.lit(\"pemex_prod\"))\n",
    "    \n",
    "\n",
    "    minmax = df.agg(F.min(\"FECHA\").alias(\"minf\"), F.max(\"FECHA\").alias(\"maxf\")).collect()[0]\n",
    "    minf, maxf = minmax[\"minf\"], minmax[\"maxf\"]\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {silver_table} (\n",
    "      FECHA DATE, ANIO INT, MES INT,\n",
    "      POZO STRING, PRODUCCION_MBD DOUBLE,\n",
    "      FLAG_NULL INT, FLAG_ZERO INT,\n",
    "      FLAG_OUTLIER_IQR INT, FLAG_OUTLIER_MOM INT, FLAG_INACTIVO INT,\n",
    "      DATA_SOURCE STRING, INGESTION_TS TIMESTAMP\n",
    "    ) USING DELTA PARTITIONED BY (ANIO, MES)\n",
    "    \"\"\")\n",
    "\n",
    "    hist = spark.table(silver_table).where(\n",
    "        (F.col(\"FECHA\") >= F.add_months(F.lit(minf), -12)) & (F.col(\"FECHA\") <= F.add_months(F.lit(maxf), 0))\n",
    "    ).select(\"FECHA\",\"POZO\",\"PRODUCCION_MBD\")\n",
    "\n",
    "    # 3) Combina histÃ³rico + nuevos para calcular IQR por POZO\n",
    "    union_for_stats = df.select(\"POZO\",\"PRODUCCION_MBD\").unionByName(\n",
    "        hist.select(\"POZO\",\"PRODUCCION_MBD\")\n",
    "    )\n",
    "    stats = (union_for_stats\n",
    "        .filter(F.col(\"PRODUCCION_MBD\").isNotNull())\n",
    "        .groupBy(\"POZO\")\n",
    "        .agg(\n",
    "            F.expr(\"percentile_approx(PRODUCCION_MBD, 0.25, 1000)\").alias(\"Q1\"),\n",
    "            F.expr(\"percentile_approx(PRODUCCION_MBD, 0.75, 1000)\").alias(\"Q3\")\n",
    "        )\n",
    "        .withColumn(\"IQR\", F.col(\"Q3\") - F.col(\"Q1\"))\n",
    "        .withColumn(\"LOW_B\", F.col(\"Q1\") - 1.5*F.col(\"IQR\"))\n",
    "        .withColumn(\"HIGH_B\", F.col(\"Q3\") + 1.5*F.col(\"IQR\"))\n",
    "    )\n",
    "\n",
    "    df2 = (df.join(stats, on=\"POZO\", how=\"left\")\n",
    "      .withColumn(\"FLAG_OUTLIER_IQR\",\n",
    "        F.when(\n",
    "          (F.col(\"PRODUCCION_MBD\").isNotNull()) &\n",
    "          ((F.col(\"PRODUCCION_MBD\") < F.col(\"LOW_B\")) | (F.col(\"PRODUCCION_MBD\") > F.col(\"HIGH_B\"))),\n",
    "          1\n",
    "        ).otherwise(0)\n",
    "      )\n",
    "      .drop(\"Q1\",\"Q3\",\"IQR\",\"LOW_B\",\"HIGH_B\")\n",
    "    )\n",
    "\n",
    "\n",
    "    prev = (spark.table(silver_table)\n",
    "        .select(\"POZO\",\"FECHA\",\"PRODUCCION_MBD\")\n",
    "        .withColumnRenamed(\"FECHA\",\"FECHA_PREV\")\n",
    "        .withColumnRenamed(\"PRODUCCION_MBD\",\"PROD_PREV\")\n",
    "    )\n",
    "    # Tomamos el mes anterior exacto (FECHA - 1 mes)\n",
    "    df3 = (df2\n",
    "      .withColumn(\"FECHA_PREV\", F.add_months(\"FECHA\", -1))\n",
    "      .join(prev, on=[\"POZO\",\"FECHA_PREV\"], how=\"left\")\n",
    "      .withColumn(\"PCT_CHANGE\",\n",
    "          F.when(F.col(\"PROD_PREV\").isNull(), None)\n",
    "           .otherwise(F.abs(F.col(\"PRODUCCION_MBD\") - F.col(\"PROD_PREV\")) /\n",
    "                      F.when(F.col(\"PROD_PREV\") == 0, None).otherwise(F.col(\"PROD_PREV\")))\n",
    "      )\n",
    "      .withColumn(\"FLAG_OUTLIER_MOM\", F.when(F.col(\"PCT_CHANGE\") > F.lit(2.0), 1).otherwise(0))\n",
    "    )\n",
    "\n",
    "    # 5) Inactividad: rachas de ceros\n",
    "    hist_for_streak = spark.table(silver_table).select(\"POZO\",\"FECHA\",\"PRODUCCION_MBD\")\n",
    "    union_for_streak = (hist_for_streak.unionByName(df3.select(\"POZO\",\"FECHA\",\"PRODUCCION_MBD\"))\n",
    "        .dropDuplicates([\"POZO\",\"FECHA\"])\n",
    "    )\n",
    "\n",
    "    w = Window.partitionBy(\"POZO\").orderBy(\"FECHA\")\n",
    "    # Creamos grupos de \"rachas\" separando por cambios cero->no cero\n",
    "    tmp = (union_for_streak\n",
    "        .withColumn(\"IS_ZERO\", F.when(F.col(\"PRODUCCION_MBD\")==0, 1).otherwise(0))\n",
    "        .withColumn(\"grp\", F.sum(F.when(F.col(\"IS_ZERO\")==0, 1).otherwise(0)).over(w))\n",
    "    )\n",
    "    wgrp = Window.partitionBy(\"POZO\",\"grp\")\n",
    "    streaked = (tmp\n",
    "        .withColumn(\"STREAK_ZERO\", F.sum(\"IS_ZERO\").over(wgrp))\n",
    "        .select(\"POZO\",\"FECHA\",\"STREAK_ZERO\")\n",
    "    )\n",
    "\n",
    "    df4 = (df3.join(streaked, on=[\"POZO\",\"FECHA\"], how=\"left\")\n",
    "        .withColumn(\"FLAG_INACTIVO\", F.when(F.col(\"STREAK_ZERO\") >= F.lit(6), 1).otherwise(0)) # 6 meses\n",
    "        .drop(\"STREAK_ZERO\",\"FECHA_PREV\",\"PROD_PREV\",\"PCT_CHANGE\")\n",
    "    )\n",
    "\n",
    "    # 6) Campos de control + particiones\n",
    "    out = (df4\n",
    "        .withColumn(\"ANIO\", F.year(\"FECHA\"))\n",
    "        .withColumn(\"MES\", F.month(\"FECHA\"))\n",
    "        .withColumn(\"INGESTION_TS\", F.current_timestamp())\n",
    "        .select(\"FECHA\",\"ANIO\",\"MES\",\"POZO\",\"PRODUCCION_MBD\",\n",
    "                \"FLAG_NULL\",\"FLAG_ZERO\",\"FLAG_OUTLIER_IQR\",\"FLAG_OUTLIER_MOM\",\"FLAG_INACTIVO\",\n",
    "                \"DATA_SOURCE\",\"INGESTION_TS\")\n",
    "    )\n",
    "\n",
    "    # 7) MERGE (upsert) a Silver \n",
    "    out.createOrReplaceTempView(\"silver_upserts\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {silver_table} AS t\n",
    "    USING silver_upserts AS s\n",
    "    ON t.FECHA = s.FECHA AND t.POZO = s.POZO\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "    return\n",
    "\n",
    "# Stream desde BRONZE con foreachBatch\n",
    "bronze_stream2silver = (\n",
    "  spark.readStream.table('pmx_etl.bronze_pmx_prod')\n",
    "   .writeStream\n",
    "   .foreachBatch(process_silver)\n",
    "   .option(\"checkpointLocation\", checkpoint_silver)\n",
    "   .trigger(availableNow=True)  \n",
    "   .start()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf2138c-f400-49c4-ae80-96b440ab71c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.rm(\"/Volumes/workspace/pmx_etl/checkpoints_silver\", recurse=True)\n",
    "#spark.sql(\"\"\"\n",
    "#          TRUNCATE TABLE workspace.pmx_etl.bronze_pmx_prod\n",
    "#          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc211324-e1e3-43fd-9b11-20d02204c237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5331424280285880,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
