{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f3dcbf-c223-45c8-9d50-114f9ca58934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f429d87c-1ead-46de-921e-f0c58216e91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Routes\n",
    "input_path = \"/Volumes/workspace/default/data_stage/\"\n",
    "checkpoint = \"/Volumes/workspace/default/checkpoints/bronze/data_prod_cp\"\n",
    "schema_location = \"/Volumes/workspace/default/schema/bronze/data_prod_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159c03c8-0f0b-4a5b-861d-6c44d916dc4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create volumes for landing, checkpointing and schema tracking\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.data_stage COMMENT 'landing_zone';\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.checkpoints COMMENT 'Checkpoint storage';\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.schema_tracking COMMENT 'Schema tracking for Auto Loader';\n",
    "\n",
    "-- Create delta table for bronze table \n",
    "CREATE TABLE IF NOT EXISTS workspace.default.bronze_pmx_prod\n",
    "USING DELTA;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d243408f-8dcc-41d9-9284-4d08d1977ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Landing to Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3f49f2-8bf8-4211-9e69-e2b47ae15a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read streaming table with cloudFiles Format\n",
    "df_raw_stream = (\n",
    "    spark.readStream.format(\"cloudFiles\")\\\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\") #Infer column type for new columns \n",
    "    .option(\"cloudFiles.schemaLocation\",\"/Volumes/workspace/default/schema_tracking\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") #Add new columns to schema\n",
    "    .load(input_path)\n",
    ")\n",
    "\n",
    "raw_cols = df_raw_stream.columns #Get columns from raw table\n",
    "\n",
    "\n",
    "df_rename_col = df_raw_stream\\\n",
    "    .select(\n",
    "        [col(c).alias(re.sub(r\"\\s+\", \"_\", re.sub(r\"\\s*\\(.*\\)\", \"\", c).strip())) for c in raw_cols] # Rename columns using regex\n",
    "    )\n",
    "\n",
    "# Adding new columns for date and ingestion date\n",
    "df_transformed = (\n",
    "    df_rename_col\n",
    "    .withColumn(\"FECHA\", to_date(col(\"FECHA\"), \"yyyy/MM\"))\n",
    "    .withColumn(\"ingestion_date\", current_timestamp()) \n",
    "    .withColumn(\"year\", year(col(\"FECHA\")))\n",
    "    .withColumn(\"month\", month(col(\"FECHA\")))\n",
    ")\n",
    "\n",
    "\n",
    "# write stream\n",
    "query = (\n",
    "    df_transformed.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/workspace/default/checkpoints\") \n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(once=True) \n",
    "    .outputMode(\"append\")\n",
    "    .table(\"default.bronze_pmx_prod\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f581ff3f-0aa2-4b40-8265-f1885179c34c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Bronze to Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa64e8ed-a29d-485a-bbe1-47c4dec50bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.readStream.table(\"default.bronze_pmx_prod\")\n",
    "display(df_bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4504468508231383,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL-PROCESS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
