{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f3dcbf-c223-45c8-9d50-114f9ca58934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f429d87c-1ead-46de-921e-f0c58216e91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Routes\n",
    "input_path = \"/Volumes/workspace/default/data_stage/\"\n",
    "checkpoint = \"/Volumes/workspace/default/checkpoints/bronze/data_prod_cp\"\n",
    "schema_location = \"/Volumes/workspace/default/schema/bronze/data_prod_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159c03c8-0f0b-4a5b-861d-6c44d916dc4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create volumes for landing, checkpointing and schema tracking\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.data_stage COMMENT 'landing_zone';\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.checkpoints COMMENT 'Checkpoint storage';\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.schema_tracking COMMENT 'Schema tracking for Auto Loader';\n",
    "\n",
    "-- Create delta table for bronze table \n",
    "CREATE TABLE IF NOT EXISTS workspace.default.bronze_pmx_prod\n",
    "USING DELTA;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d243408f-8dcc-41d9-9284-4d08d1977ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Landing to Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3f49f2-8bf8-4211-9e69-e2b47ae15a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read streaming table with cloudFiles Format\n",
    "df_raw_stream = (\n",
    "    spark.readStream.format(\"cloudFiles\")\\\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\") #Infer column type for new columns \n",
    "    .option(\"cloudFiles.schemaLocation\",\"/Volumes/workspace/default/schema_tracking\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") #Add new columns to schema\n",
    "    .load(input_path)\n",
    ")\n",
    "\n",
    "\n",
    "#Normalize columns\n",
    "def normalize_cols(df_raw_stream,cols):\n",
    "    df_rename_col = df_raw_stream\\\n",
    "        .select(\n",
    "            [col(c).alias(re.sub(r\"\\s+\", \"_\", re.sub(r\"\\s*\\(.*\\)\", \"\", c).strip())) for c in raw_cols] # Rename columns using regex\n",
    "        )\n",
    "    return df_rename_col\n",
    "\n",
    "# Adding new columns for date and ingestion date\n",
    "def add_ingestion_date(df_rename_col):\n",
    "    return df_rename_col.withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "\n",
    "raw_cols = df_raw_stream.columns #Get columns from raw table\n",
    "df_rename = normalize_cols(df_raw_stream,raw_cols)\n",
    "df_transformed = add_ingestion_date(df_rename)\n",
    "\n",
    "# write stream\n",
    "query = (\n",
    "    df_transformed.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/workspace/default/checkpoints\") \n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(once=True) \n",
    "    .outputMode(\"append\")\n",
    "    .table(\"default.bronze_pmx_prod\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f581ff3f-0aa2-4b40-8265-f1885179c34c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Bronze to Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e65fd50-c13c-4971-836f-cf340cb1c660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.readStream.table(\"default.bronze_pmx_prod\")\n",
    "\n",
    "# Create functions for normalize table and unpivot\n",
    "def normalize_fecha(df):\n",
    "    return (df\n",
    "        .withColumn(\"_ym\", to_date(concat_ws(\"-\", col(\"FECHA\"), lit(\"01\")), \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"FECHA\", last_day(col(\"_ym\")))\n",
    "        .drop(\"_ym\")\n",
    "    )\n",
    "def unpivot_to_long(df_normalize):\n",
    "    cols = [c for c in df_normalize.columns if c != \"FECHA\"]\n",
    "    N = len(cols)\n",
    "    pairs = \", \".join([f\"'{c}', `{c}`\" for c in cols])\n",
    "    unpivot_df = df_normalize\\\n",
    "        .select(\n",
    "            col(\"FECHA\"),\n",
    "            expr(f\"stack({N}, {pairs}) AS (POZO, PRODUCION_MBD)\")\n",
    "        )\n",
    "\n",
    "    return unpivot_df\n",
    "\n",
    "def add_flags(df_unpivot):\n",
    "    withColumn(\"PRODUCION_MBD\", col(\"PRODUCION_MBD\").cast(\"double\"))\\\n",
    "    .withColumn(\"FLAG_NULL\", when(col(\"PRODUCION_MBD\")).IsNull(),1).otherwise(0)\\\n",
    "    .withColumn(\"FLAG_ZERO\", when(col(\"PRODUCION_MBD\") == 0),1).otherwise(0)\n",
    "    return df_unpivot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34488107-ad68-413c-82c4-5de4fbaa2aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS default.silver_pmx_prod (\n",
    "  FECHA DATE,\n",
    "  ANIO INT,\n",
    "  MES INT,\n",
    "  POZO STRING,\n",
    "  PRODUCCION_MBD DOUBLE,\n",
    "  FLAG_NULL INT,\n",
    "  FLAG_ZERO INT,\n",
    "  FLAG_OUTLIER_IQR INT,\n",
    "  FLAG_OUTLIER_MOM INT,\n",
    "  FLAG_INACTIVO INT,\n",
    "  DATA_SOURCE STRING,\n",
    "  INGESTION_TS TIMESTAMP\n",
    ") USING DELTA PARTITIONED BY (ANIO, MES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b32bb97-bf1a-4cd8-b736-b991c7bf2073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_silver(microDF, batchId):\n",
    "    if microDF.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Normalize and unpivot table    \n",
    "    df_normalize = normalize_fecha(microDF)\n",
    "    df_unpivot = unpivot_to_long(df_normalize)\n",
    "    df = add_flags(df_unpivot)\n",
    "\n",
    "    #Collect min and max date in order to filter historic table\n",
    "    minmax = df.agg(min(\"FECHA\").alias(\"minf\"),max(\"FECHA\").alias(\"maxf\")).collect()[0]\n",
    "    minf, maxf = minmax[\"minf\"], minmax[\"maxf\"]\n",
    "\n",
    "    hist = spark.table(silver_table).filter(\n",
    "        (col(\"FECHA\") >= add_months(lit(minf), -12)) & (col(\"FECHA\") <= add_months(lit(maxf), 0))\n",
    "    ).select(\"POZO\", \"FECHA\", \"PRODUCION_MBD\")\n",
    "\n",
    "    #unionByName hist + new data \n",
    "    union_for_stats = df.select(\"POZO\", \"PRODUCCION_MBD\").unionByName(\n",
    "        hist.select(\"POZO\",\"PRODUCCION_MBD\")\n",
    "    )\n",
    "\n",
    "    #Calculate IQR in order to add flags \n",
    "    stats = (\n",
    "        union_for_stats\n",
    "        .filter(col(\"PRODUCCION_MBD\").isNotNull())\n",
    "        .groupBy(\"POZO\")\n",
    "        .agg(\n",
    "            expr(\"percentile_approx(PRODUCCION_MBD, 0.25, 1000)\").alias(\"Q1\"),\n",
    "            expr(\"percentile_approx(PRODUCCION_MBD, 0.75, 1000)\").alias(\"Q3\")\n",
    "        )\n",
    "        .withColumn(\"IQR\", col(\"Q3\") - col(\"Q1\"))\n",
    "        .withColumn(\"LOW_B\", col(\"Q1\") - 1.5*col(\"IQR\"))\n",
    "        .withColumn(\"HIGH_B\", col(\"Q3\") + 1.5*col(\"IQR\"))\n",
    "    )\n",
    "\n",
    "    # add flags when the new data is less than lowb or mayor than highb\n",
    "    df2 = (\n",
    "        df.join(stats, on = \"POZO\", how = \"left\")\n",
    "        .withColumn(\"FLAG_OUTLIER_IQR\",\n",
    "            when(\n",
    "                (col(\"PRODUCCION_MBD\").isNotNull() &\n",
    "               ((col(\"PRODUCCION_MBD\") < col(\"LOW_B\")) | (col(\"PRODUCCION_MBD\") > col(\"HIGH_B\"))),\n",
    "                1)).otherwise(0)\n",
    "            )\n",
    "        .drop(\"Q1\",\"Q3\",\"IQR\",\"LOW_B\",\"HIGH_B\")\n",
    "    )\n",
    "\n",
    "    #Geting historic table \n",
    "    prev = (spark.table(silver_table)\n",
    "    .select(\"POZO\",\"FECHA\",\"PRODUCCION_MBD\")\n",
    "    .withColumnRenamed(\"FECHA\",\"FECHA_PREV\")\n",
    "    .withColumnRenamed(\"PRODUCCION_MBD\",\"PROD_PREV\")\n",
    "    )\n",
    "\n",
    "\n",
    "    df3 = (df2\n",
    "      .withColumn(\"FECHA_PREV\", add_months(\"FECHA\", -1))\n",
    "      .join(prev, on=[\"POZO\",\"FECHA_PREV\"], how=\"left\")\n",
    "      .withColumn(\"PCT_CHANGE\",\n",
    "        when(col(\"PROD_PREV\").isNull(), None)\n",
    "        .otherwise(abs(col(\"PRODUCCION_MBD\") - col(\"PROD_PREV\")) /\n",
    "        .when(col(\"PROD_PREV\") == 0, None).otherwise(col(\"PROD_PREV\")))\n",
    "      )\n",
    "      .withColumn(\"FLAG_OUTLIER_MOM\",\n",
    "        when(col(\"PCT_CHANGE\") > lit(2.0), 1).otherwise(0))\n",
    "    )\n",
    "\n",
    "    hist_for_streak = spark.table(silver_table).select(\"POZO\",\"FECHA\",\"PRODUCCION_MBD\")\n",
    "    union_for_streak = (hist_for_streak.unionByName(df3.select(\"POZO\",\"FECHA\",\"PRODUCCION_MBD\"))\n",
    "        .dropDuplicates([\"POZO\",\"FECHA\"])\n",
    "    )\n",
    "\n",
    "    w = Window.partitionBy(\"POZO\").orderBy(\"FECHA\")\n",
    "    tmp = (\n",
    "        union_for_streak\n",
    "            .withColumn(\"IS_ZERO\", when(col(\"PRODUCCION_MBD\")==0,1).otherwise(0))\n",
    "            .withColumn(\"grp\", sum(when(col(\"IS_ZERO\") == 0, 1).otherwise(0)).over(w))\n",
    "    )\n",
    "\n",
    "\n",
    "    wgrp = Window.partitionBy(\"POZO\",\"grp\")\n",
    "    streaked = (\n",
    "        tmp.withColumn(\"STREAK_ZERO\", sum(\"IS_ZERO\").over(wgrp))\n",
    "        .select(\"POZO\",\"FECHA\",\"STREAK_ZERO\")\n",
    "    )\n",
    "\n",
    "    df4 = (\n",
    "        df3.join(streaked, on = [\"POZO\",\"FECHA\"], how=\"left\")\n",
    "        .withColum(\"FLAG_INACTIVO\", when(col(\"STREAK_ZERO\" >= lit(6),1).otherwise(0)))\n",
    "        .drop(\"STREAK_ZERO\",\"FECHA_PREV\",\"PROD_PREV\",\"PCT_CHANGE\")\n",
    "    )\n",
    "\n",
    "    out = (\n",
    "        df4.withColumn(\"ANIO\", year(\"FECHA\"))\n",
    "        .withColumn(\"MES\", month(\"FECHA\"))\n",
    "        .withColumn(\"INGESTION_TS\", current_timestamp())\n",
    "        .select(\"FECHA\",\"ANIO\",\"MES\",\"POZO\",\"PRODUCCION_MBD\",\"FLAG_NULL\",\"FLAG_ZERO\",\n",
    "                \"FLAG_OUTLIER_IQR\",\"FLAG_OUTLIER_MOM\",\"FLAG_INACTIVO\",\n",
    "                \"DATA_SOURCE\",\"INGESTION_TS\")\n",
    "    )\n",
    "     \n",
    "    out.createOrReplaceTempView(\"silver_upserts\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {silver_table} AS t\n",
    "    USING silver_upserts as s\n",
    "    ON t.FECHA = s.FECHA AND t.POZO = s.POZO\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *           \n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "bronze_stream2silver = (\n",
    "  spark.readStream.table(\"default.bronze_pemex_prod\")\n",
    "   .writeStream\n",
    "   .foreachBatch(process_silver)\n",
    "   .option(\"checkpointLocation\", checkpoint_silver)\n",
    "   .trigger(once=True)  # o availableNow=True\n",
    "   .start()\n",
    ")\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4504468508231383,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL-PROCESS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
